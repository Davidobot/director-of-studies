# Applied methods: research design and evaluation (A-level Sociology WJEC / Eduqas)

## What this section tests and why it matters
This section of the specification asks you to design, justify and evaluate a piece of sociological research. That means the exam question will give you a research aim or topic and expect you to show how you would turn that into a workable study, explain why you chose particular methods and samples, anticipate practical and ethical problems, and evaluate the strengths and limitations of your design. The skills examined are applied and synthetic: you must demonstrate understanding of research concepts such as validity, reliability, generalisability and representativeness, and show how these concepts shape decisions about methods and sampling. Examiners want evidence that you can think like a sociologist: define terms, operationalise concepts, balance theory and practicality, and justify choices with reference to likely impacts on the quality of the data.

## From research aim to operationalisation
Every good research design begins with a clear research aim and workable research questions. A typical aim might be, for example, to investigate why some pupils in Welsh secondary schools perform less well in GCSEs than others. Operationalisation is the process of translating abstract concepts — such as “educational engagement” or “social class” — into measurable indicators. Be precise: if you operationalise social class using parental occupation, explain why that measure is appropriate and what it does not capture (for instance, cultural capital). If you operationalise “educational engagement” as attendance rates, homework completion and pupil self-reports, justify the mix and note limitations. A good exam answer sets out the main variables, explains how each will be measured, and shows awareness that operationalisation involves trade-offs between accuracy, feasibility and ethics.

## Choosing methods and justifying them
Your choice of methods should flow from the research aim and from your priorities among validity, reliability, generalisability and depth of understanding. Quantitative methods such as structured questionnaires and large-scale surveys are strong where you need representativeness and statistical generalisability. They work well when concepts can be operationalised into fixed-response items and when you need to compare groups. Qualitative methods such as semi-structured interviews, focus groups and participant observation are stronger for depth, context and validity: they allow respondents to explain meanings and reveal unexpected issues. Ethnography is ideal when you want rich, contextualised accounts of everyday life, while longitudinal designs are necessary when studying change over time.

In applied questions you will often propose mixed methods or methodological pluralism (triangulation). Explaining how a survey might establish patterns and how interviews could then explain those patterns is a persuasive route: the survey provides breadth and the qualitative follow-up provides depth. Always justify your chosen methods in the context of the research question and consider theoretical perspectives. A positivist rationale values structured methods and objectivity, while an interpretivist justification emphasises verstehen and the need for in-depth qualitative techniques. Realist approaches can be used to justify combining methods to uncover mechanisms as well as patterns.

## Sampling: types, justification and likely problems
Sampling choices must be tied to your aims and to practical constraints. Probability sampling (simple random, stratified random, systematic) supports claims about generalisability and representativeness, so if your aim is to estimate the prevalence of an attitude across Welsh school pupils, a stratified random sample that ensures coverage of regions, school types and socio-economic strata is appropriate. Explain how you would create a sampling frame and how strata would be chosen. Non-probability sampling (purposive, snowball, volunteer, quota) is acceptable when studying hard-to-reach groups or when depth rather than generalisability is the priority, for example when researching a youth subculture.

Discuss sample size and response rate. A large sample increases statistical power for quantitative work but costs more and is harder to manage; a small purposive sample allows intensive qualitative analysis but limits generalisability. Anticipate problems such as non-response bias, gatekeeper refusal (for schools or institutions), and sampling frame incompleteness. Offer practical solutions: pilot the questionnaire, use multiple contact attempts, provide incentives, negotiate with gatekeepers and use snowballing where access is difficult. Always evaluate how these solutions might introduce new biases, for instance incentives possibly encouraging untruthful responses.

## Ethics and practical issues: anticipate and mitigate
Ethical considerations are central. You must address informed consent, confidentiality and anonymity, the right to withdraw, protection from harm, and the handling of sensitive information. With vulnerable groups (young people, disabled participants) explain how parental consent, age-appropriate information sheets and safeguarding measures would be used. If your design includes covert observation, acknowledge the ethical problem of deception and justify or reject it: covert methods can improve validity in some contexts but are rarely acceptable without strong justification and ethical approval.

Practical issues include time, cost, access and operationalisation of measures. Time limits may rule out long-term ethnography; budget constraints might favor online questionnaires rather than postal surveys. Access problems at institutional gates can be managed through formal letters, ethical approval from the school or institution, and offering feedback to participants as a reward. Operationalisation problems — for example, ambiguous survey items or culturally biased measures — can be reduced by piloting instruments and consulting with gatekeepers. Always explain trade-offs: a secure, ethically robust design might reduce sample size or introduce delays, and you should show how you would balance these demands.

## Data collection, presentation and analysis
Explain how you would collect and present data in ways appropriate to the method. For quantitative studies, describe the questionnaire delivery method (online, face-to-face, postal), likely response formats and any coding plans. For analysis, mention descriptive statistics (means, percentages), cross-tabulations to explore relationships, and the use of simple inferential statistics if appropriate. For qualitative studies, describe recording procedures, transcription, coding and thematic analysis. Explain how you would present qualitative findings using verbatim extracts linked to analytical commentary, and how themes would be developed and evidenced. When using mixed methods, show how you would integrate findings: for example, use survey results to identify patterns and then illustrate those patterns with interview quotations.

Consider how to present results visually. Explain when tables and bar charts are useful for showing group comparisons, when scatterplots best show relationships, and when thematic maps or concept diagrams might clarify qualitative findings. Be explicit about how the chosen presentation helps answer the research question and aids evaluation: a well-presented table may reveal patterns that a narrative alone cannot.

## Evaluation: strengths, limitations and improvements
Evaluation is particularly important in this part of the specification. You should assess your design against the core methodological criteria: validity, reliability, generalisability, representativeness, objectivity and ethical soundness. For each criterion, explain the extent to which your choices support or weaken it. For example, a structured questionnaire may offer high reliability and ease of replication but lower validity if questions are superficial; semi-structured interviews may provide high validity and depth but lower reliability and generalisability.

Discuss likely biases and how they would affect conclusions. Consider respondent effects (social desirability), researcher effects (observer bias, reflexivity), sampling bias and measurement error. Suggest realistic improvements: pilot studies to improve question wording, training for interviewers to reduce variation, triangulation to corroborate evidence, or a mixed-methods sequential design that uses qualitative findings to refine quantitative instruments. Also consider the wider social and political context: access to official statistics may be limited or delayed, and policy changes can alter the relevance of your operationalisation.

Finally, show awareness of how theoretical perspectives shape evaluation. A positivist critique will focus on measurement error and sample representativeness, while an interpretivist critique emphasizes the depth and authenticity of meanings gained from participants. Demonstrating these different vantage points strengthens your evaluation and shows examiners you can link methods to sociological theory.

## Exam technique
When the paper asks you to design research, structure your answer clearly and follow the logical steps a researcher would take: state the research aim, operationalise key concepts, describe and justify your chosen methods, explain sampling and access arrangements, address ethical and practical issues, outline data presentation and analysis, and finish with a balanced evaluation. Use specific examples to make your points concrete — for instance, name the particular sampling technique you would use for a study of Welsh secondary schools, or sketch the kinds of survey questions you would pilot. Quantify where useful: estimate sample sizes and explain why they are sufficient, or suggest likely timeframes and costs in broad terms. Always link assessments back to sociological concepts such as validity, reliability and generalisability, and show awareness of theoretical implications. Time your answer so you can include a thoughtful evaluation; examiners give marks for justification and critique as much as for the design itself.